{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM, RobertaTokenizer, RobertaModel\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast(\"../ScilitBERT/ScilitBERT_tokenizer/scilitBERT_tok-vocab.json\",\"../ScilitBERT/ScilitBERT_tokenizer/scilitBERT_tok-merges.txt\")\n",
    "\n",
    "ScilitBERT = RobertaForMaskedLM.from_pretrained(\"../ScilitBERT/ScilitBERT_cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a problem with the tokenizer it does not tokenize <mask> as the special mask token, ...\n",
    "\n",
    "def replace_masks(sentance):\n",
    "    inputs = ScilitBERT_tokenizer(sentance)\n",
    "    new_inputs=[]\n",
    "    new_mask=[]\n",
    "    mask_index=-1\n",
    "    for i,tok in enumerate(inputs[\"input_ids\"]):\n",
    "        if tok==34 and inputs[\"input_ids\"][i-1]==44174 and inputs[\"input_ids\"][i-2]==1388:\n",
    "            new_inputs=new_inputs[:-2]\n",
    "            new_mask=new_mask[:-2]\n",
    "            new_inputs.append(4)\n",
    "            new_mask.append(0)\n",
    "            mask_index=len(new_inputs)-1\n",
    "        else:  \n",
    "            new_inputs.append(tok)\n",
    "            new_mask.append(1)\n",
    "    return {\"input_ids\":torch.IntTensor(new_inputs).view(-1,len(new_inputs)), \"attention_mask\":torch.IntTensor(new_mask).view(-1,len(new_mask))}, mask_index\n",
    "\n",
    "def softmax(logits: list)-> list:\n",
    "    proba = [np.exp(i) for i in logits]\n",
    "    total = sum(proba)\n",
    "    proba = [i / total for i in proba]\n",
    "    return proba\n",
    "\n",
    "def top_predict_masked_token(sentance, nb_pred):\n",
    "    inputs, mask_index=replace_masks(sentance)\n",
    "    outputs = ScilitBERT(**inputs)\n",
    "    logits = outputs.logits.detach().numpy()\n",
    "    logits=softmax(logits[0][mask_index])\n",
    "    max_indexes=sorted(range(len(logits)), key=lambda i: logits[i])[-nb_pred:]\n",
    "    max_indexes.reverse()\n",
    "    proba=[logits[i] for i in max_indexes]\n",
    "    res=ScilitBERT_tokenizer.convert_ids_to_tokens(max_indexes)\n",
    "    return res , proba\n",
    "\n",
    "def display_output(sentance, nb_pred):\n",
    "    words,proba=top_predict_masked_token(sentance,nb_pred)\n",
    "    df=pd.DataFrame({\"word\":words,\"proba\":proba})\n",
    "    print(f\"completions for sentance:{sentance}\")\n",
    "    display(df.round(2))"
   ]
  },
  {
   "source": [
    "## Some examples"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "completions for sentance:a language model can be pretrained then <mask>-tuned on a downstream task\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "    word  proba\n0  Ġfine   0.96\n1    Ġre   0.01\n2  Ġhand   0.01",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>proba</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ġfine</td>\n      <td>0.96</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Ġre</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ġhand</td>\n      <td>0.01</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "display_output(\"a language model can be pretrained then <mask>-tuned on a downstream task\",3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "completions for sentance:The <mask> are located on the valence layer\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "          word  proba\n0   Ġelectrons   0.31\n1   Ġparticles   0.05\n2  Ġelectrodes   0.05",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>proba</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ġelectrons</td>\n      <td>0.31</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Ġparticles</td>\n      <td>0.05</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ġelectrodes</td>\n      <td>0.05</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "display_output(\"The <mask> are located on the valence layer\",3)\n",
    "# True answer: electrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "completions for sentance: Unlike recent language representation models, BERT is designed to pre-train deep <mask> representations from unlabeled text\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "        word  proba\n0  Ġlanguage   0.29\n1      Ġtext   0.16\n2  Ġsemantic   0.12",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>proba</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ġlanguage</td>\n      <td>0.29</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Ġtext</td>\n      <td>0.16</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ġsemantic</td>\n      <td>0.12</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "display_output(\" Unlike recent language representation models, BERT is designed to pre-train deep <mask> representations from unlabeled text\",3)\n",
    "# True answer: bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "completions for sentance:The Masked Language Modelling <mask> enables the representation to fuse the left and the right context.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "         word  proba\n0  Ġframework   0.24\n1   Ġapproach   0.11\n2  Ġtechnique   0.10",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>proba</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ġframework</td>\n      <td>0.24</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Ġapproach</td>\n      <td>0.11</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ġtechnique</td>\n      <td>0.10</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "display_output(\"The Masked Language Modelling <mask> enables the representation to fuse the left and the right context.\",3)\n",
    "# True answer: objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "completions for sentance:vaccine was reported to have an efficacy of 94.1% at preventing symptomatic COVID-19 due to infection with ‘wild-type’ variants in a randomized <mask> trial.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "          word  proba\n0  Ġcontrolled   0.58\n1    Ġclinical   0.35\n2     Ġcontrol   0.03",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>proba</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ġcontrolled</td>\n      <td>0.58</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Ġclinical</td>\n      <td>0.35</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ġcontrol</td>\n      <td>0.03</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "display_output(\"vaccine was reported to have an efficacy of 94.1% at preventing symptomatic COVID-19 due to infection with ‘wild-type’ variants in a randomized <mask> trial.\",3)\n",
    "# True Answer: clinical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "completions for sentance:vaccine was <mask> to have an efficacy of 94.1% at preventing symptomatic COVID-19 due to infection with ‘wild-type’ variants in a randomized clinical trial.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "          word  proba\n0       Ġfound   0.25\n1  Ġdetermined   0.19\n2       Ġshown   0.11",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>proba</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ġfound</td>\n      <td>0.25</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Ġdetermined</td>\n      <td>0.19</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ġshown</td>\n      <td>0.11</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "display_output(\"vaccine was <mask> to have an efficacy of 94.1% at preventing symptomatic COVID-19 due to infection with ‘wild-type’ variants in a randomized clinical trial.\",3)\n",
    "#True answer: reported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38864bitconda50f9a214ca5e4c89bc58cc336f92ac61",
   "display_name": "Python 3.8.8 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}